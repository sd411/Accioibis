{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5okE57yaAvd",
    "outputId": "c3eb030f-8153-42d2-b65d-8e5a494589f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyY7ye3BaTTt",
    "outputId": "19423b00-478a-4500-b233-b04ceef8523f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.2% 1649.2/1662.8MB downloaded\n",
      "/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "909j6wz_azLS"
   },
   "outputs": [],
   "source": [
    "model_path ='/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzwRwuCSa27B",
    "outputId": "621dfad2-2115-447c-a34c-1a7f9939fae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5MB 4.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=b3b145e5c10ba0686e5457907574e4e8049d5875b448e1e1e11a1b6dffc5a120\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.10\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 5.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 22.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 26.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=a0ec764c2e278b2443236005eb4a3f7378a4fb14e0f73648d164b52ea6e20573\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n",
      "Requirement already up-to-date: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.4)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (51.0.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 4.2MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "!pip install python-docx\n",
    "!pip install transformers\n",
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade tensorflow\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "KRW_FL9wbzrU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cfgLdv_ellxH"
   },
   "outputs": [],
   "source": [
    "class DocSim:\n",
    "    def __init__(self, w2v_model, stopwords=None):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords if stopwords is not None else []\n",
    "\n",
    "    def vectorize(self, doc: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Identify the vector values for each word in the given document\n",
    "        :param doc:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        # PS: There are other & better ways to do it.\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "\n",
    "    def calculate_similarity(self, source_doc, target_docs=None, threshold=0):\n",
    "        \"\"\"Calculates & returns similarity scores between given source document & all\n",
    "        the target documents.\"\"\"\n",
    "        if not target_docs:\n",
    "            return []\n",
    "\n",
    "        if isinstance(target_docs, str):\n",
    "            target_docs = [target_docs]\n",
    "\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        results = []\n",
    "        for doc in target_docs:\n",
    "            target_vec = self.vectorize(doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                results.append( sim_score)\n",
    "            # Sort results by score in desc order\n",
    "            #results.sort(key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DvXTYGUrb7UX"
   },
   "outputs": [],
   "source": [
    "stopwords_path=\"drive/MyDrive/cohesion/stopwords.txt\"\n",
    "with open(stopwords_path, 'r') as fh:\n",
    "    stopwords = fh.read().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TIxLMwYMb-_p"
   },
   "outputs": [],
   "source": [
    "ds = DocSim(w2v_model,stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234,
     "referenced_widgets": [
      "f13945251b194c5087a4e5b6a5773a60",
      "e193236f4d5346269d62113c993f7dd7",
      "c0d7b0d125454ff38c8469f9d48e0e3a",
      "b03f5aa4faac442abefe165ed5836127",
      "2453eadf886c4678bc1b259ac9872ebe",
      "5de6396dfd7c487681d50e765a4ce211",
      "562c5e7aadc84078bca3b8d88a38d526",
      "34779d010c294b97ab39d3592a55d955",
      "c70fcc5049d74c34b197fc85deeca9bb",
      "8bc4e2e42b38493faab5b742e90f14ec",
      "16d3ef866235478fa6d5fe21bf50f8ea",
      "9ad8595069e4457a9091537e194e6e2e",
      "f09f1cf0861a45d6a08d4e2ab0402a52",
      "0118752e30ad45f98910faedb93f96bc",
      "5aa5239b48ab4e6b98f1c153e5a8ca3e",
      "b26a72036ec941ee99ad9594f4532d1c",
      "ddc00a9db70740f0b3c06ed1f9be4de5",
      "ddb56ead7b00433293b5082c663b7066",
      "ab3249658de646d98a9af59755f11b84",
      "dc529ff2668645d0b9512a6530c1f911",
      "0860b8d360b7464fa6aaecba57125ed1",
      "e26d10c7bfde464e9aecbf38c76a0f56",
      "e4b68a53cb444240bb112c34cfeb19fe",
      "40a49bbadba84e6e80ace834e1f8d087"
     ]
    },
    "id": "xe8gnNFtcBRs",
    "outputId": "23719596-e078-40d3-86c4-5fb38031a69e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13945251b194c5087a4e5b6a5773a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70fcc5049d74c34b197fc85deeca9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-small were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc00a9db70740f0b3c06ed1f9be4de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GKSslmCzcEl6"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BnMnzZDVcLNy"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xmbnuTc_cMC8"
   },
   "outputs": [],
   "source": [
    "def countX(lst, x): \n",
    "    return lst.count(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "j2NuA_NSPLDy"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "NP0z9C2UqA14"
   },
   "outputs": [],
   "source": [
    "def gin(prob,paragraph):\n",
    "  paar=paragraph.replace(\"\\n\\n\",\"\\n\")\n",
    "  \n",
    "  para=paar.split(\"\\n\")\n",
    "  \n",
    "  while '' in para:para.remove('')\n",
    "    #para.remove('')\n",
    "    #para.remove(' ')\n",
    "  #except Exception:\n",
    "    #abc=len(para)\n",
    "  abc=len(para)\n",
    "  #print(para)\n",
    "  #print(abc)\n",
    "  #print(prob)\n",
    "  lines=[]\n",
    "  lines_res=[]\n",
    "  for j in range(0,abc):\n",
    "    z=para[j]\n",
    "    y=z.split(\".\")\n",
    "    ee=y[0]\n",
    "  #xe=y[1].join(y[:])\n",
    "    lines.append(ee)\n",
    "    for e in y:\n",
    "      s=1\n",
    "      le=len(y)\n",
    "      att=''.join(y[1:le])\n",
    "    lines_res.append(att)\n",
    "  #print(lines_res)\n",
    "\n",
    "  summarys=[]\n",
    "  for i in para:\n",
    "      text=i\n",
    "      preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "      t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "      #print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "      tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "      # summmarize \n",
    "      summary_ids = model.generate(tokenized_text,\n",
    "                                      num_beams=4,\n",
    "                                      no_repeat_ngram_size=2,\n",
    "                                      min_length=30,\n",
    "                                      max_length=100,\n",
    "                                      early_stopping=True)\n",
    "\n",
    "      output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "      summarys.append(output)\n",
    "\n",
    "  texts=prob\n",
    "  preprocess_texts = texts.strip().replace(\"\\n\",\"\")\n",
    "  t5_prepared_Texts = \"summarize: \"+preprocess_texts\n",
    "      #print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "  tokenized_texts = tokenizer.encode(t5_prepared_Texts, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "      # summmarize \n",
    "  summary_idss = model.generate(tokenized_texts,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=100,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "  outputs = tokenizer.decode(summary_idss[0], skip_special_tokens=True)\n",
    "  #print(outputs)\n",
    " #first line of para1\n",
    "  texts_first=lines[0]\n",
    "  preprocess_texts_first = texts_first.strip().replace(\"\\n\",\"\")\n",
    "  t5_prepared_Texts_first = \"summarize: \"+preprocess_texts_first\n",
    "      #print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "  tokenized_texts_first = tokenizer.encode(t5_prepared_Texts_first, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "      # summmarize \n",
    "  summary_idsss = model.generate(tokenized_texts_first,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=100,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "  outputs_firstline = tokenizer.decode(summary_idsss[0], skip_special_tokens=True)\n",
    "  #print(outputs_firstline)\n",
    "#remaining line of para1\n",
    "  texts_rest=lines_res[0]\n",
    "  preprocess_texts_rest = texts_rest.strip().replace(\"\\n\",\"\")\n",
    "  t5_prepared_Texts_rest = \"summarize: \"+preprocess_texts_rest\n",
    "      #print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "  tokenized_texts_rest = tokenizer.encode(t5_prepared_Texts_rest, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "      # summmarize \n",
    "  summary_idssss = model.generate(tokenized_texts_rest,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=100,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "  outputs_rest = tokenizer.decode(summary_idssss[0], skip_special_tokens=True)\n",
    "  #print(lines_res[0])\n",
    "  #print(outputs_rest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  lines_sum=[]\n",
    "  lines_res_sum=[]\n",
    "  for c in range(0,abc):\n",
    "    w=summarys[c]\n",
    "    yy=w.split(\".\")\n",
    "    eee=yy[0]\n",
    "    #xe=y[1].join(y[:])\n",
    "    lines_sum.append(eee)\n",
    "    for es in yy:\n",
    "      ss=1\n",
    "      lee=len(yy)\n",
    "      attt=''.join(yy[1:lee])\n",
    "    lines_res_sum.append(attt)\n",
    "  #print(summarys[0])\n",
    " # print(lines_sum)\n",
    "  #print(lines_res_sum)\n",
    "  #while '' in lines_sum:lines_sum.remove('')\n",
    "  #while '' in lines_res_sum:lines_res_sum.remove('')\n",
    "\n",
    "  sim_scores1=ds.calculate_similarity(outputs, outputs_firstline)\n",
    "  sim_scores2=ds.calculate_similarity(outputs, outputs_rest)\n",
    "  try:\n",
    "    sim_scores3=ds.calculate_similarity(outputs, summarys[1])\n",
    "  except Exception as e:\n",
    "    sim_scores3=0\n",
    "  try:\n",
    "    sim_scores4=ds.calculate_similarity(outputs, summarys[2])\n",
    "  except Exception as e:\n",
    "    sim_scores4=0\n",
    "  try:\n",
    "    sim_scores5=ds.calculate_similarity(summarys[1], summarys[2])\n",
    "  except Exception as e:\n",
    "    sim_scores5=0\n",
    "##linking words\n",
    "\n",
    "  document1 = Document('drive/MyDrive/cohesion/link.docx')\n",
    "  link=[]\n",
    "  hello=['','\\xa0']\n",
    "  for paras in document1.paragraphs:\n",
    "      if paras.text not in hello:\n",
    "      #if paras.text != '':\n",
    "        link.append(paras.text)\n",
    "\n",
    "\n",
    "  length=len(para)\n",
    "  words=[]\n",
    "  parass=[]\n",
    "  for ss in range(0,length):\n",
    "      essay=para[ss]\n",
    "      for j in link:\n",
    "          if re.search(j,essay,re.IGNORECASE):\n",
    "              words.append(j)\n",
    "              parass.append(ss)\n",
    "    \n",
    "    #valu=Counter(parass)\n",
    "\n",
    "  coo=[]\n",
    "  for co in range(0,length):\n",
    "    coo.append(countX(parass,co))\n",
    "  a_list=zip(words,parass)\n",
    "  final_list=list(a_list)\n",
    "  df=pd.DataFrame()\n",
    "  df['Words']=words\n",
    "  df['Para_number']=parass\n",
    "  tot_num=df.Para_number.count()\n",
    "\n",
    "  #results1=[]\n",
    "  #results2=[]\n",
    "  try:\n",
    "    results1=[]\n",
    "    for me in coo:\n",
    "      #res_avg=(me/tot_num)\n",
    "      res1=(me/tot_num)**2\n",
    "      results1.append(res1)\n",
    "      #results2.append(res_avg)\n",
    "    total=sum(results1)\n",
    "    total_avg= tot_num/(length)\n",
    "  except Exception as e:\n",
    "    total=0\n",
    "    total_avg=0\n",
    "\n",
    "\n",
    "##reference words\n",
    "  document2 = Document('drive/MyDrive/cohesion/reference.docx')\n",
    "  reference=[]\n",
    "  hello=['','\\xa0']\n",
    "  for paras in document2.paragraphs:\n",
    "      if paras.text not in hello:\n",
    "      #if paras.text != '':\n",
    "        reference.append(paras.text)\n",
    "\n",
    "  length=len(para)\n",
    "  wordsr=[]\n",
    "  parassr=[]\n",
    "  for ssr in range(0,length):\n",
    "      essayr=para[ssr]\n",
    "      for jr in reference:\n",
    "          if re.search(jr,essayr,re.IGNORECASE):\n",
    "              wordsr.append(jr)\n",
    "              parassr.append(ssr)\n",
    "    \n",
    "    #valu=Counter(parass)\n",
    "  coor=[]\n",
    "  for cor in range(0,length):\n",
    "    coor.append(countX(parassr,cor))\n",
    "  a_listr=zip(wordsr,parassr)\n",
    "  final_listr=list(a_listr)\n",
    "  dfr=pd.DataFrame()\n",
    "  dfr['Words']=wordsr\n",
    "  dfr['Para_number']=parassr\n",
    "  tot_numr=dfr.Para_number.count()\n",
    "\n",
    "  try:\n",
    "    results2=[]\n",
    "    for mer in coor:\n",
    "      res2=(mer/tot_numr)**2\n",
    "      results2.append(res2)\n",
    "    totalr=sum(results2)\n",
    "    avg_ref= tot_numr/(length)\n",
    "    countr=dfr.groupby(['Para_number']).size().reset_index(name='counts')\n",
    "  except Exception as e:\n",
    "    totalr=0\n",
    "    countr=0\n",
    "\n",
    "#conclusion \n",
    "  document3 = Document('drive/MyDrive/cohesion/conclusion.docx')\n",
    "  concl=[]\n",
    "  hello=['','\\xa0']\n",
    "  for paras in document3.paragraphs:\n",
    "      if paras.text not in hello:\n",
    "      #if paras.text != '':\n",
    "        concl.append(paras.text)\n",
    "  try:\n",
    "    sim_scores6 = ds.calculate_similarity(summarys[abc-1],outputs_rest)\n",
    "  except Exception as e:\n",
    "    sim_scores6=0\n",
    "  #length=len(para)\n",
    "  wordsc=[]\n",
    "  parassc=[]\n",
    "  essayc=para[-1]\n",
    "  #no=essayc.split()\n",
    "  #jj=len(no)\n",
    "  try:\n",
    "    for jc in concl:\n",
    "        if re.search(jc,essayc,re.IGNORECASE):\n",
    "                wordsc.append(jc)\n",
    "                  #parassc.append(ssc)\n",
    "    #print(wordsc)\n",
    "    no_concl=len(wordsc)\n",
    "  except Exception as e:\n",
    "    no_concl=0\n",
    "\n",
    "  #print(para[0])\n",
    "  #print(lines[0])\n",
    "  #print(lines_res[0])\n",
    "  #print(outputs_firstline)\n",
    "  #print(outputs_rest)\n",
    "  #return exe\n",
    "  results=[abc,str(sim_scores1),str(sim_scores2),str(sim_scores3),str(sim_scores4),str(sim_scores5),str(no_concl),str(sim_scores6),str(total_avg),str(total),str(avg_ref),str(totalr)]\n",
    "  #results=[abc,a1,b1,c1,d1,e1,str(no_concl),f1,str(total_avg),str(total),str(avg_ref),str(totalr)]\n",
    "  return results\n",
    "\n",
    "\n",
    "  #print(para[0])\n",
    "  #print(lines[0])\n",
    "  #print(lines_res[0])\n",
    " # print(outputs_firstline)\n",
    "  #print(outputs_rest)\n",
    "  #print(lines_sum)\n",
    "  #print(lines_res_sum)\n",
    "  #print(sim_scores1)\n",
    "  #print(sim_scores2)\n",
    "  #print(sim_scores3)\n",
    "  #print(sim_scores4)\n",
    "  #print(sim_scores5)\n",
    "  #print(jj)\n",
    "\n",
    "  #print(outputs)\n",
    "\n",
    "#be=prob,ae=para\n",
    "#rr=gin(be,ae)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X6vYn0ZWTkkV",
    "outputId": "b0fd50f5-e444-4893-e510-7634400c5c46"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Code</td>\n",
       "      <td>Question</td>\n",
       "      <td>Essay</td>\n",
       "      <td>Overall Score</td>\n",
       "      <td>CC</td>\n",
       "      <td>Lex</td>\n",
       "      <td>Grammar</td>\n",
       "      <td>Task Achv</td>\n",
       "      <td>No of Paragraph</td>\n",
       "      <td>Similarity Score (Problem Statement, 1st Sente...</td>\n",
       "      <td>Similarity Score (Problem Statement, Remaining...</td>\n",
       "      <td>Similarity Score (Problem Statement, Para-2)</td>\n",
       "      <td>Similarity Score (Problem Statement, Para-3)</td>\n",
       "      <td>Similarity Score (Para-2, Para-3)</td>\n",
       "      <td>Presence of Concluding words/phrases</td>\n",
       "      <td>Similarity Score (Last Para, Remaining part of...</td>\n",
       "      <td>Avg Linking word per para</td>\n",
       "      <td>HHE Score Linking Words</td>\n",
       "      <td>Avg Reference word per para</td>\n",
       "      <td>HHE Score Reference Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IC1</td>\n",
       "      <td>Some people believe that technology has made m...</td>\n",
       "      <td>Experts throughout both the developing and dev...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IC2</td>\n",
       "      <td>People are using a lot of online language tran...</td>\n",
       "      <td>The importance and popularity of web-based lan...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IC3</td>\n",
       "      <td>These days some people spend a lot of money on...</td>\n",
       "      <td>The issue of whether to spend one’s hard earne...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IC4</td>\n",
       "      <td>Online education and training is becoming incr...</td>\n",
       "      <td>Enthusiasm for digital technology is at an all...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>AA196</td>\n",
       "      <td>Modern societies need specialists in certain f...</td>\n",
       "      <td>They tell me; Love never dies!\\nI ask them; Do...</td>\n",
       "      <td>7</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>AA197</td>\n",
       "      <td>How do you think nuclear technologies will evo...</td>\n",
       "      <td>from my prespective, i think that the utilizat...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>AA198</td>\n",
       "      <td>what is those good means something really not ...</td>\n",
       "      <td>One year access to the entire PrepScholar prog...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>AA199</td>\n",
       "      <td>A foreign visitor has only one day to spend in...</td>\n",
       "      <td>If I want to talk about my country that will t...</td>\n",
       "      <td>6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>AA200</td>\n",
       "      <td>nowadays children are watching too much t v..w...</td>\n",
       "      <td>In this modern era, technology has influenced ...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  ...                Unnamed: 19\n",
       "0         Code  ...  HHE Score Reference Words\n",
       "1          IC1  ...                        NaN\n",
       "2          IC2  ...                        NaN\n",
       "3          IC3  ...                        NaN\n",
       "4          IC4  ...                        NaN\n",
       "..         ...  ...                        ...\n",
       "291      AA196  ...                        NaN\n",
       "292      AA197  ...                        NaN\n",
       "293      AA198  ...                        NaN\n",
       "294      AA199  ...                        NaN\n",
       "295      AA200  ...                        NaN\n",
       "\n",
       "[296 rows x 20 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(\"drive/MyDrive/cohesion/CC_ANN_Model.xlsx\")\n",
    "df\n",
    "#len(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "MPEnIFuncPRR"
   },
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPxZCINSXoeV"
   },
   "outputs": [],
   "source": [
    "ra=range(1,51)\n",
    "fin=[]\n",
    "for i in ra:\n",
    "  prob=df['Unnamed: 1'][i]\n",
    "  paras=df['Unnamed: 2'][i]\n",
    "  resultss=gin(prob,paras)\n",
    "  fin.append(resultss)\n",
    "#print(fin)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9oC3MViAWbi",
    "outputId": "fd1a3006-c5f1-48da-b4b6-46c23c1b5bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is undoubtedly that the world encounters the new challenge of enlarged need for nourishment in comparison with the last century. This state might have two grounds - the huge increase of population all over the world and notable climate changes such as global warming. Despite the current status of this concern it might be rectified by several â€œrescueâ€ programs in the manner of a proper field cultivation or using state-of-the-art technologies in agricultural and biological sciences.\n",
      "\tFood demand is meant to increase anywhere up to 98 percent by 2050. This may become a cause for changing markets that no one has seen before. In fact, farmers and businesses did not manage to catch it up, so they faced the low offerings at the time of tremendous demand. Unfortunately, the climate changes have also contributed negatively, so that it has led to a reduction in possible food production pace. There are a variety of forms of them from global warming caused by high CO2 emissions up to a depletion of atmosphere layers.\n",
      "\tIn spite of the aforementioned statements, the rising demand for food caused by human population growth is supposed to be resolved through mirror measures namely an increase of the amount of agricultural land to grow crops or an enhancement of productivity on current agricultural fields through fertilizer and irrigation and adopting new methods like accuracy farming. In addition to these steps, there might be room for improvement in existing technologies in the genetic science field. If genetically modified foods are cheaper than the ordinary ones, why do mankind stick to the traditional approach? Certainly, they do not need to, and as an illustration, in 2018 international scientists made a breakthrough in GM (Genetically modified) foods manufacturing making them worth less in 5 times. That example has shown that GM food will ensure that anyone will access a healthy and inexpensive meal everyday despite his financial state.\n",
      "\tTo conclude, the increasing demand for nutriment raised compared to an age back since a number of people became greater and climate alterations become noticeable. The measures which the community may take to respond to the needs consist of usage of quality cultivation and fertilizer along with adoption of new methods, and an option to full employment of GM food in life.\n"
     ]
    }
   ],
   "source": [
    "#print(str(df['Unnamed: 2'][71]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOaMRK-TCJpH",
    "outputId": "f1250fe2-74c7-4a47-a35c-a841f4ce0739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, '[0.54174495]', '[0.56553113]', '[0.72577715]', '[0.51141906]', '[0.5595054]', '1', '[0.68777305]', '3.25', '0.2781065088757397', '4.5', '0.2777777777777778']]\n"
     ]
    }
   ],
   "source": [
    "print(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lnDk0TnOrX-A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QfqVv7arebo",
    "outputId": "39c0cccb-1841-450c-adc4-564306648631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[0.578091]', '[0.7225043]', '[0.67608]', '[0.54487336]', '[0.25872934]', '[0.6841996]', '[0.3885041]', '[0.5217158]', '[0.40795892]', '[0.12117803]', '[0.6023805]', '[0.5624409]', '[0.6701355]', '[0.28957674]', '[0.4614207]', '[0.37801525]', '[0.23402713]', '[]', '[0.6727857]', '[0.5278691]', '[]', '[0.39966443]', '[0.59919316]', '[0.28310516]', '[0.3654935]', '[0.47415784]', '[0.8546635]', '[0.2584248]', '[0.8541606]', '[]', '[0.59675163]', '[]', '[0.8404949]', '[0.52642524]', '[0.46929082]', '[0.42316654]', '[]', '[0.95697767]', '[0.4017174]', '[0.44837576]', '[0.112542726]', '[0.5388785]', '[0.27483836]', '[0.46498215]', '[0.18961617]']\n"
     ]
    }
   ],
   "source": [
    "par_s=[]\n",
    "sim_sco1=[]\n",
    "sim_sco2=[]\n",
    "sim_sco3=[]\n",
    "sim_sco4=[]\n",
    "sim_sco5=[]\n",
    "concl_wo=[]\n",
    "sim_sco6=[]\n",
    "avg_lin=[]\n",
    "hhe_link=[]\n",
    "avg_ref=[]\n",
    "hhe_ref=[]\n",
    "\n",
    "for i in fin:\n",
    "  par_s.append(i[0])\n",
    "  sim_sco1.append(i[1])\n",
    "  sim_sco2.append(i[2])\n",
    "  sim_sco3.append(i[3])\n",
    "  sim_sco4.append(i[4])\n",
    "  sim_sco5.append(i[5])\n",
    "  concl_wo.append(i[6])\n",
    "  sim_sco6.append(i[7])\n",
    "  avg_lin.append(i[8])\n",
    "  hhe_link.append(i[9])\n",
    "  avg_ref.append(i[10])\n",
    "  hhe_ref.append(i[11])\n",
    "\n",
    "print(sim_sco2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "2E35iJEk4aUf"
   },
   "outputs": [],
   "source": [
    "df_new=pd.DataFrame()\n",
    "df_new['Code']=df['Unnamed: 0'][251:296]\n",
    "df_new['Question']=df['Unnamed: 1'][251:296]\n",
    "df_new['Essay']=df['Unnamed: 2'][251:296]\n",
    "df_new['Overall score']=df['Unnamed: 3'][251:296]\n",
    "df_new['CC']=df['Unnamed: 4'][251:296]\n",
    "df_new['Lex']=df['Unnamed: 5'][251:296]\n",
    "df_new['Grammer']=df['Unnamed: 6'][251:296]\n",
    "df_new['task achv']=df['Unnamed: 7'][251:296]\n",
    "df_new['No of paragraph']=par_s\n",
    "df_new['sim score(problem statement & 1st sentence']=sim_sco1\n",
    "df_new['Similarity Score (Problem Statement, Remaining part of Para-1']=sim_sco2\n",
    "df_new['Similarity Score (Problem Statement, Para-2)']=sim_sco3\n",
    "df_new['Similarity Score (Problem Statement, Para-3)']=sim_sco4\n",
    "df_new['Similarity Score (Para-2, Para-3)']=sim_sco5\n",
    "df_new['Presence of Concluding words/phrases']=concl_wo\n",
    "df_new['Similarity Score (Last Para, Remaining part of Para-1)']=sim_sco6\n",
    "df_new['Avg Linking word per para']=avg_lin\n",
    "df_new['HHE Score Linking Words']=hhe_link\n",
    "df_new['Avg Reference word per para']=avg_ref\n",
    "df_new['HHE Score Reference Words']=hhe_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "t6GzqU9tzEqS"
   },
   "outputs": [],
   "source": [
    "df_new\n",
    "df_new.to_excel(\"drive/MyDrive/cohesion/update/final/output_final_1-51.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "EVej13Jxv9NG",
    "outputId": "a971099a-be9a-484c-e990-0a150645edbb"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-d6faa14d583c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mparas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mresultss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(fin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7f898a5c61fd>\u001b[0m in \u001b[0;36mgin\u001b[0;34m(prob, paragraph)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mpara\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mabc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#print(para)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#print(abc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLKEtTys1IXv",
    "outputId": "bf79290f-8882-4cee-9813-cd5bd37997f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, '[0.4385118]', '[0.44848222]', '0', '0', '0', '3', '0', '13.0', '1.0', '7.0', '1.0']]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "8HVMJ85AZJK1",
    "outputId": "cd9c9362-dff9-44b9-e2d3-0c03c95831cd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n\\nPeople learn and develop throughout their entire lives. I think that in our modern world it is very essential to be familiar with computer technology. So, if I had a chance to give a child a gift it would be a computer. I think that computers play an essential role in our lives and they bring many benefits to our society. Moreover, children can learn by use of computers. In the following paragraphs I will give my reasons to support my answer.\\n\\nFirst of all, by use of computers children can play many games, which help to improve children's ability to think logically, think about their next step, etc. Moreover, playing games develop many important qualities such as attention, patience, persistent, etc. Second of all, computers help children to learn more about anything by use of Internet. They can find new friends even from another country. Children will improve their communication skills, gain more knowledge and experience. Also, children have a great opportunity to learn more about other countries, their history, traditions and customs. Finally, computer skills can help a child to find his or her first job. A child can find an ad in the Internet about a job offer or he or she can make a resume and place it in the Internet. Personally, I think it is a great experience and big step forward towards a future career.\\n\\nIn addition to those practical benefits, computer technology helps children to do their homework faster. They can type their data into the computer, easily check the grammar, correct mistakes and then print it out. Moreover, there are plenty different kinds of educational programs that can help children learn how to read, write, draw and even how to behave and speak a foreign language. \\n \\nTo sum up, I believe that children should learn how to use computer because this knowledge will help them in the future to be more self-confident and enterprising. Furthermore, computers can greatly improve and simplify their lives if children know how to use them.\\n\\n\""
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH_K-R0CO2Li"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "X62FXussfgNi"
   },
   "outputs": [],
   "source": [
    "def gini(prob,paragraph):\n",
    "  para=paragraph.split(\".\\n\")\n",
    "  abc=len(para)\n",
    "  print(para,\"\\n\")\n",
    "  print(abc)\n",
    "  #print(prob)\n",
    "  lines=[]\n",
    "  lines_res=[]\n",
    "  for j in range(0,abc):\n",
    "    z=para[j]\n",
    "    y=z.split(\".\")\n",
    "    ee=y[0]\n",
    "  #xe=y[1].join(y[:])\n",
    "    lines.append(ee)\n",
    "    for e in y:\n",
    "      s=1\n",
    "      le=len(y)\n",
    "      att=''.join(y[1:le])\n",
    "    lines_res.append(att)\n",
    "  #print(lines_res)\n",
    "\n",
    "  summarys=[]\n",
    "  for i in para:\n",
    "      text=i\n",
    "      preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "      t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "      #print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "      tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "      # summmarize \n",
    "      summary_ids = model.generate(tokenized_text,\n",
    "                                      num_beams=4,\n",
    "                                      no_repeat_ngram_size=2,\n",
    "                                      min_length=30,\n",
    "                                      max_length=100,\n",
    "                                      early_stopping=True)\n",
    "\n",
    "      output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "      summarys.append(output)\n",
    "\n",
    "  texts=prob\n",
    "  preprocess_texts = texts.strip().replace(\".\",\"\")\n",
    "  t5_prepared_Texts = \"summarize: \"+preprocess_texts\n",
    "      #print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "  tokenized_texts = tokenizer.encode(t5_prepared_Texts, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "      # summmarize \n",
    "  summary_idss = model.generate(tokenized_texts,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=100,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "  outputs = tokenizer.decode(summary_idss[0], skip_special_tokens=True)\n",
    "  print(output)\n",
    "  lines_sum=[]\n",
    "  lines_res_sum=[]\n",
    "  for c in range(0,abc):\n",
    "    w=summarys[c]\n",
    "    yy=w.split(\".\")\n",
    "    eee=yy[0]\n",
    "    #xe=y[1].join(y[:])\n",
    "    lines_sum.append(eee)\n",
    "    for es in yy:\n",
    "      ss=1\n",
    "      lee=len(yy)\n",
    "      attt=''.join(yy[1:lee])\n",
    "    lines_res_sum.append(attt)\n",
    "\n",
    "  sim_scores1=ds.calculate_similarity(outputs, lines_sum[0])\n",
    "  sim_scores2=ds.calculate_similarity(outputs, lines_res_sum[0])\n",
    "  try:\n",
    "    sim_scores3=ds.calculate_similarity(outputs, summarys[1])\n",
    "  except Exception as e:\n",
    "    sim_scores3=0\n",
    "  try:\n",
    "    sim_scores4=ds.calculate_similarity(outputs, summarys[2])\n",
    "  except Exception as e:\n",
    "    sim_scores4=0\n",
    "  try:\n",
    "    sim_scores5=ds.calculate_similarity(summarys[1], summarys[2])\n",
    "  except Exception as e:\n",
    "    sim_scores5=0\n",
    "##linking words\n",
    "\n",
    "  document1 = Document('drive/MyDrive/cohesion/link.docx')\n",
    "  link=[]\n",
    "  hello=['','\\xa0']\n",
    "  for paras in document1.paragraphs:\n",
    "      if paras.text not in hello:\n",
    "      #if paras.text != '':\n",
    "        link.append(paras.text)\n",
    "\n",
    "\n",
    "  length=len(para)\n",
    "  words=[]\n",
    "  parass=[]\n",
    "  for ss in range(0,length):\n",
    "      essay=para[ss]\n",
    "      for j in link:\n",
    "          if re.search(j,essay,re.IGNORECASE):\n",
    "              words.append(j)\n",
    "              parass.append(ss)\n",
    "    \n",
    "    #valu=Counter(parass)\n",
    "\n",
    "  coo=[]\n",
    "  for co in range(0,length):\n",
    "    coo.append(countX(parass,co))\n",
    "  a_list=zip(words,parass)\n",
    "  final_list=list(a_list)\n",
    "  df=pd.DataFrame()\n",
    "  df['Words']=words\n",
    "  df['Para_number']=parass\n",
    "  tot_num=df.Para_number.count()\n",
    "\n",
    "  #results1=[]\n",
    "  #results2=[]\n",
    "  try:\n",
    "    results1=[]\n",
    "    for me in coo:\n",
    "      #res_avg=(me/tot_num)\n",
    "      res1=(me/tot_num)**2\n",
    "      results1.append(res1)\n",
    "      #results2.append(res_avg)\n",
    "    total=sum(results1)\n",
    "    total_avg= tot_num/(length)\n",
    "  except Exception as e:\n",
    "    total=0\n",
    "    total_avg=0\n",
    "\n",
    "\n",
    "##reference words\n",
    "  document2 = Document('drive/MyDrive/cohesion/reference.docx')\n",
    "  reference=[]\n",
    "  hello=['','\\xa0']\n",
    "  for paras in document2.paragraphs:\n",
    "      if paras.text not in hello:\n",
    "      #if paras.text != '':\n",
    "        reference.append(paras.text)\n",
    "\n",
    "  length=len(para)\n",
    "  wordsr=[]\n",
    "  parassr=[]\n",
    "  for ssr in range(0,length):\n",
    "      essayr=para[ssr]\n",
    "      for jr in reference:\n",
    "          if re.search(jr,essayr,re.IGNORECASE):\n",
    "              wordsr.append(jr)\n",
    "              parassr.append(ssr)\n",
    "    \n",
    "    #valu=Counter(parass)\n",
    "  coor=[]\n",
    "  for cor in range(0,length):\n",
    "    coor.append(countX(parassr,cor))\n",
    "  a_listr=zip(wordsr,parassr)\n",
    "  final_listr=list(a_listr)\n",
    "  dfr=pd.DataFrame()\n",
    "  dfr['Words']=wordsr\n",
    "  dfr['Para_number']=parassr\n",
    "  tot_numr=dfr.Para_number.count()\n",
    "\n",
    "  try:\n",
    "    results2=[]\n",
    "    for mer in coor:\n",
    "      res2=(mer/tot_numr)**2\n",
    "      results2.append(res2)\n",
    "    totalr=sum(results2)\n",
    "    avg_ref= tot_numr/(length)\n",
    "    countr=dfr.groupby(['Para_number']).size().reset_index(name='counts')\n",
    "  except Exception as e:\n",
    "    totalr=0\n",
    "    countr=0\n",
    "\n",
    "#conclusion \n",
    "  document3 = Document('drive/MyDrive/cohesion/conclusion.docx')\n",
    "  concl=[]\n",
    "  hello=['','\\xa0']\n",
    "  for paras in document3.paragraphs:\n",
    "      if paras.text not in hello:\n",
    "      #if paras.text != '':\n",
    "        concl.append(paras.text)\n",
    "  try:\n",
    "    sim_scores6 = ds.calculate_similarity(summarys[3],lines_res_sum[0])\n",
    "  except Exception as e:\n",
    "    sim_scores6=0\n",
    "  #length=len(para)\n",
    "  wordsc=[]\n",
    "  parassc=[]\n",
    "  essayc=para[-1]\n",
    "  #no=essayc.split()\n",
    "  #jj=len(no)\n",
    "  try:\n",
    "    for jc in concl:\n",
    "        if re.search(jc,essayc,re.IGNORECASE):\n",
    "                wordsc.append(jc)\n",
    "                  #parassc.append(ssc)\n",
    "    #print(wordsc)\n",
    "    no_concl=len(wordsc)\n",
    "  except Exception as e:\n",
    "    no_concl=0\n",
    "\n",
    "  \n",
    "  #return exe\n",
    "  results=[abc,str(sim_scores1),str(sim_scores2),str(sim_scores3),str(sim_scores4),str(sim_scores5),str(no_concl),str(sim_scores6),str(total_avg),str(total),str(avg_ref),str(totalr)]\n",
    "  #results=[abc,a1,b1,c1,d1,e1,str(no_concl),f1,str(total_avg),str(total),str(avg_ref),str(totalr)]\n",
    "  return results\n",
    "\n",
    "\n",
    "  #print(summarys)\n",
    "  #print(lines_sum)\n",
    "  #print(lines_res_sum)\n",
    "  #print(sim_scores1)\n",
    "  #print(sim_scores2)\n",
    "  #print(sim_scores3)\n",
    "  #print(sim_scores4)\n",
    "  #print(sim_scores5)\n",
    "  #print(jj)\n",
    "\n",
    "  #print(outputs)\n",
    "\n",
    "#be=prob,ae=para\n",
    "#rr=gin(be,ae)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuYTIa0Giuep"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnUWLCWJUZt0",
    "outputId": "67edc397-6489-4acc-b80d-f7ad150eead3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It is undoubtedly that the world encounters the new challenge of enlarged need for nourishment in comparison with the last century. This state might have two grounds - the huge increase of population all over the world and notable climate changes such as global warming. Despite the current status of this concern it might be rectified by several â€œrescueâ€\\x9d programs in the manner of a proper field cultivation or using state-of-the-art technologies in agricultural and biological sciences', '\\tFood demand is meant to increase anywhere up to 98 percent by 2050. This may become a cause for changing markets that no one has seen before. In fact, farmers and businesses did not manage to catch it up, so they faced the low offerings at the time of tremendous demand. Unfortunately, the climate changes have also contributed negatively, so that it has led to a reduction in possible food production pace. There are a variety of forms of them from global warming caused by high CO2 emissions up to a depletion of atmosphere layers', '\\tIn spite of the aforementioned statements, the rising demand for food caused by human population growth is supposed to be resolved through mirror measures namely an increase of the amount of agricultural land to grow crops or an enhancement of productivity on current agricultural fields through fertilizer and irrigation and adopting new methods like accuracy farming. In addition to these steps, there might be room for improvement in existing technologies in the genetic science field. If genetically modified foods are cheaper than the ordinary ones, why do mankind stick to the traditional approach? Certainly, they do not need to, and as an illustration, in 2018 international scientists made a breakthrough in GM (Genetically modified) foods manufacturing making them worth less in 5 times. That example has shown that GM food will ensure that anyone will access a healthy and inexpensive meal everyday despite his financial state', '\\tTo conclude, the increasing demand for nutriment raised compared to an age back since a number of people became greater and climate alterations become noticeable. The measures which the community may take to respond to the needs consist of usage of quality cultivation and fertilizer along with adoption of new methods, and an option to full employment of GM food in life.'] \n",
      "\n",
      "4\n",
      "the growing demand for nutriment raised compared to an age back since a number of people became greater and climate alterations become noticeable. the measures consist of use of quality cultivation and fertilizer along with adoption of new methods, and an option to full employment of GM food in life.\n"
     ]
    }
   ],
   "source": [
    "ra=range(71,72)\n",
    "fin=[]\n",
    "for i in ra:\n",
    "  prob=df['Unnamed: 1'][i]\n",
    "  paras=df['Unnamed: 2'][i]\n",
    "  resultss=gini(prob,paras)\n",
    "  fin.append(resultss)\n",
    "#print(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT3AGRPlO2S_",
    "outputId": "35faf3fe-e136-4554-d1b9-f91a949e3061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Experts throughout both the developing and developed world have debated whether the advent of sophisticated modern technology such as mobile phones, laptops and iPad have helped to enhance and improve people’s social lives or whether the opposite has become the case. Personally, I strongly advocate the former view. This essay will discuss both sides using examples from the UK government and Oxford University to demonstrate points and prove arguments', 'On the one hand there is ample, powerful, almost daily evidence that such technology can be detrimental especially to the younger generation who are more easily affected by it’s addictive nature and which can result in people feeling more isolated from the society. The central reason behind this is twofold, firstly, the invention of online social media sites and apps, such as Twitter and Facebook have reduced crucial face-to-face interactions dramatically. Through use of these appealing and attractive mediums, people feel in touch and connected yet lack key social skills and the ability to communicate.  Secondly, dependence on such devices is built up frighteningly easily which may have a damaging effect on mental health and encourage a sedentary lifestyle. For example, recent scientific research by the UK government demonstrated that 90% of people in their 30s spend over 20 hours per week on Messenger and similar applications to chat with their friends instead of meeting up and spending quality time together or doing sport. As a result, it is conclusively clear that these technology advancements have decreased and diminished our real life interactions', 'On the other hand, although there are significant downsides to technological developments, its’ multifold advantages cannot be denied. This is largely because the popularity of technology such as cellphones allows people to connect freely and easily with no geographical barriers. People are able to share any type of news, information, photos and opinions with their loved ones whenever and wherever they want therefore keeping a feeling of proximity and closeness. For example, an extensive study by Oxford University illustrated that people who work, or study abroad and use applications like Facetime and WhatsApp to chat with their families, are less likely to experience loneliness and feel out of the loop than those  who do not. Consistent with this line of thinking is that businessmen are also undoubtedly able to benefit from these advances by holding virtual real -time meetings using Skype which may increase the chance of closing business deals without the need to fly', 'From the arguments and examples given I firmly believe that overall communication and mans’ sociability has been advanced enormously due to huge the huge technological progress of the past twenty years and despite some potentially serious health implications which governments should not fail to address, it is predicted that its popularity will continue to flourish in the future.'] \n",
      "\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ra=range(1,2)\n",
    "fin=[]\n",
    "for i in ra:\n",
    "  prob=df['Unnamed: 1'][i]\n",
    "  paras=df['Unnamed: 2'][i]\n",
    "  resultss=gini(prob,paras)\n",
    "  fin.append(resultss)\n",
    "#print(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b56tZohnivgP",
    "outputId": "b4b1f730-8693-45ca-dee1-1fa8c2a9f74e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaching the true information has been our concern since the beginning of human life. Since then, many scientists have investigated many concepts in different fields of life.  The collection of this knowledge has brought a dilemma that gathers scientists and business people to think about in which degree to mention information in their research or business-related articles. I personally think that it might be crucial to share a lot of information in business world while it is redundant to do it in an academic matter. The current essay will discuss this statement. \n",
      "To begin with, the main feature of being a leader in a business market is to be international as well as being interactive. So that,  companies can be aware of new ideas, trends and strategies to develop the quality and quantity of their work. That is why they need to process a lot of information to create new designs and maybe become unique in their market. \n",
      "On the other hand, the academic world needs specific information to reach new concepts and theories. For example,  when I was carrying out research with some topics from positive psychology, I only needed to cite some key information because of the purpose of the research. If I mentioned further information other than I needed, that would be unnecessary and time-consuming for readers. Therefore, it is more appropriate to use a simple and clear flow in academic papers instead of sharing a lot of information that makes it complex. \n",
      "In conclusion, the business world needs different innovative ideas to make progression in their work, but the academic world needs a specific type of data to contribute on their research. Thus, preferring to mention different kinds of information in business-related articles may be more relevant rather than in scientific papers. It is not suprising that many scientific journals have specific word limits for research papers. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ii=(df['Unnamed: 2'][202])\n",
    "if \"\\n\" in ii:\n",
    "  print(str(ii))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "parameter_excel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0118752e30ad45f98910faedb93f96bc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0860b8d360b7464fa6aaecba57125ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "16d3ef866235478fa6d5fe21bf50f8ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0118752e30ad45f98910faedb93f96bc",
      "max": 242065649,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f09f1cf0861a45d6a08d4e2ab0402a52",
      "value": 242065649
     }
    },
    "2453eadf886c4678bc1b259ac9872ebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "34779d010c294b97ab39d3592a55d955": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40a49bbadba84e6e80ace834e1f8d087": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "562c5e7aadc84078bca3b8d88a38d526": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5aa5239b48ab4e6b98f1c153e5a8ca3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5de6396dfd7c487681d50e765a4ce211": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bc4e2e42b38493faab5b742e90f14ec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ad8595069e4457a9091537e194e6e2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b26a72036ec941ee99ad9594f4532d1c",
      "placeholder": "​",
      "style": "IPY_MODEL_5aa5239b48ab4e6b98f1c153e5a8ca3e",
      "value": " 242M/242M [00:06&lt;00:00, 39.2MB/s]"
     }
    },
    "ab3249658de646d98a9af59755f11b84": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e26d10c7bfde464e9aecbf38c76a0f56",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0860b8d360b7464fa6aaecba57125ed1",
      "value": 791656
     }
    },
    "b03f5aa4faac442abefe165ed5836127": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34779d010c294b97ab39d3592a55d955",
      "placeholder": "​",
      "style": "IPY_MODEL_562c5e7aadc84078bca3b8d88a38d526",
      "value": " 1.20k/1.20k [00:00&lt;00:00, 1.26kB/s]"
     }
    },
    "b26a72036ec941ee99ad9594f4532d1c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0d7b0d125454ff38c8469f9d48e0e3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5de6396dfd7c487681d50e765a4ce211",
      "max": 1197,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2453eadf886c4678bc1b259ac9872ebe",
      "value": 1197
     }
    },
    "c70fcc5049d74c34b197fc85deeca9bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16d3ef866235478fa6d5fe21bf50f8ea",
       "IPY_MODEL_9ad8595069e4457a9091537e194e6e2e"
      ],
      "layout": "IPY_MODEL_8bc4e2e42b38493faab5b742e90f14ec"
     }
    },
    "dc529ff2668645d0b9512a6530c1f911": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40a49bbadba84e6e80ace834e1f8d087",
      "placeholder": "​",
      "style": "IPY_MODEL_e4b68a53cb444240bb112c34cfeb19fe",
      "value": " 792k/792k [00:13&lt;00:00, 58.5kB/s]"
     }
    },
    "ddb56ead7b00433293b5082c663b7066": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddc00a9db70740f0b3c06ed1f9be4de5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab3249658de646d98a9af59755f11b84",
       "IPY_MODEL_dc529ff2668645d0b9512a6530c1f911"
      ],
      "layout": "IPY_MODEL_ddb56ead7b00433293b5082c663b7066"
     }
    },
    "e193236f4d5346269d62113c993f7dd7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e26d10c7bfde464e9aecbf38c76a0f56": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4b68a53cb444240bb112c34cfeb19fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f09f1cf0861a45d6a08d4e2ab0402a52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f13945251b194c5087a4e5b6a5773a60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c0d7b0d125454ff38c8469f9d48e0e3a",
       "IPY_MODEL_b03f5aa4faac442abefe165ed5836127"
      ],
      "layout": "IPY_MODEL_e193236f4d5346269d62113c993f7dd7"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
